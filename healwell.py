# -*- coding: utf-8 -*-
"""HealWell.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vc_k2xKZJGPWButs95XvzFs7BbDTWEPw
"""

#HealWell

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#connect with google drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/

# Read the CSV file
df = pd.read_csv('Stress-Lysis.csv')

df.head()

df.shape #to specify the no. of rows and columns

df.info #concise summary (data types,null,etc.)

df.describe() #statistical summary of the data

#DATA CLEANING

# Step 1 - #converting categorical data into numerical data

df['Stress_Level'].unique() #returns unique values found in column

df.isnull().sum() #returns the number of missing values in each cell

df.duplicated().sum() #used to count the number of duplicated rows

#EDA - Exploratory Data Analysis(investigating dataset to summarize its main characteristics)

#check for outliners. if present try to handle them.

df.skew() #<0 -> data is negatively skewed (left) , >0 -> data is positively skewed (right)

#creating a boxplot/whisker plot of the data
plt.figure(figsize=(6,4))
plt.boxplot(df)
plt.show()

#Skewness Reduction - All 4 columns consists skewness

#Skewness Reduction for the humidity column
Humidity_log=np.log(df['Humidity'])
h=round(Humidity_log.skew(),10)
print(h)

Humidity_log.skew()

#Skewness Reduction for the temperature column
Temperature_sqrt=np.sqrt(df['Temperature'])
t=round(Temperature_sqrt.skew(),10)
print(t)

#Quartile range calculation to handle outliers

quantile1 = df["Step_count"].quantile(0.25)
quantile2 = df["Step_count"].quantile(0.75)

quantile1

quantile2

#the data should lie in the range of 50.0 and 150.0

df["Step_count"]=np.where(df["Step_count"]<quantile1,quantile1,df["Step_count"])
df["Step_count"]=np.where(df["Step_count"]>quantile2,quantile2,df["Step_count"])

s = round(df["Step_count"].skew(),10)
print(s)

stress_sqrt = np.sqrt(df['Stress_Level'])
sl = round(stress_sqrt.skew(),10)
print(sl)

column=['Step_count','Humidity']
for category in column:
    plt.figure(figsize=(3,3))
    plt.hist(df[category])
    plt.title(category)
    plt.show()

# histplot (categorical)
plt.figure(figsize=(3, 3))
sns.set(font_scale=1)
sns.countplot(data=df, x='Stress_Level')
plt.title('Distribution of Stress Levels')
plt.show()

plt.figure(figsize=(3,3))
sns.boxplot(df['Temperature'])

#correlation

correlation=df.corr() #correlation between each column
correlation

plt.figure(figsize=(3,3))
sns.heatmap(correlation,annot=True,cmap='crest',linewidths=0.2)
plt.show()

plt.figure(figsize=(3,3))
sns.scatterplot(x='Humidity',y='Temperature',hue='Stress_Level',data=df)
plt.show()

#MODELLING

#Logistic Regression

from sklearn.model_selection import train_test_split

X = df.drop(['Stress_Level'], axis=1)
y = df['Stress_Level']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

from sklearn.linear_model import LogisticRegression
regressor=LogisticRegression(C=1.0,random_state=2)
regressor.fit(X_train,y_train)

from sklearn.metrics import accuracy_score, confusion_matrix
prediction = regressor.predict(X_test)
confusionmatrix = confusion_matrix(y_test,prediction) #confusion matrix tells the number of correctly predicted elements
print(confusionmatrix)

print(accuracy_score(y_test,prediction))

#random forest classification

from sklearn.ensemble import RandomForestClassifier
model=RandomForestClassifier(n_estimators=100,max_depth=3,random_state=0)
model.fit(X,y)
prediction = model.predict(X_test)
confusionmatrix = confusion_matrix(y_test,prediction)
print(confusionmatrix)

print(accuracy_score(y_test,prediction))

#Support Vector Machine

from sklearn.preprocessing import StandardScaler

# Create an instance of the StandardScaler
standardscaler = StandardScaler()

# Fit and transform the training data
X_train_scaled = standardscaler.fit_transform(X_train)

# Transform the test data using the same scaler
X_test_scaled = standardscaler.transform(X_test)

from sklearn.svm import SVC
classifier = SVC(kernel='linear', random_state=0)
classifier.fit(X_train, y_train)

y_predict= classifier.predict(X_test)
score=accuracy_score(y_test,y_predict)
print(score)

#Deployment

import pickle
filename = 'stress_trained.sav'
pickle.dump(classifier,open(filename,'wb'))

loaded_model = pickle.load(open('stress_trained.sav','rb'))

input_data = (2,9,10) #300
#changing the input data into numpy array
id_np_array = np.asarray(input_data)
id_reshaped = id_np_array.reshape(1,-1)

prediction = classifier.predict(id_reshaped)
print(prediction)

if(prediction[0]==0):
    print("Stress Level: LOW")
elif(prediction[0]==1):
    print("Stress Level: MEDIUM")
else:
    print("Stress Level: HIGH")

